<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Narender Reddy Gopu - Data Engineer</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      background-color: #333;
      color: #fff;
      padding: 20px;
      text-align: center;
    }
    .container {
      max-width: 800px;
      margin: 20px auto;
      padding: 0 20px;
      background-color: #fff;
      border-radius: 10px;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    }
    h1, h2, h3 {
      color: #333;
    }
    p {
      margin-bottom: 10px;
    }
    .section {
      margin-bottom: 30px;
    }
    .section-header {
      background-color: #333;
      color: #fff;
      padding: 10px 20px;
      border-top-left-radius: 10px;
      border-top-right-radius: 10px;
    }
    .section-body {
      padding: 20px;
    }
    .qualification-list {
      list-style-type: none;
      padding: 0;
    }
    .qualification-list li {
      margin-bottom: 10px;
    }
  </style>
</head>
<body>
  <header>
    <h1>Narender Reddy Gopu</h1>
    <p>Phone: 937-369-9454 | Email: narenderreddy0825@gmail.com</p>
  </header>
  <div class="container">
    <div class="section">
      <div class="section-header">
        <h2>Professional Summary</h2>
      </div>
      <div class="section-body">
        <p>
          Over 4 years of experience dealing with structured and unstructured data, proficiently managing tasks like Data Mining, Data Acquisition, Data Validation, Predictive Modeling, and Data Visualization. Proficient in Python ETL Development, crafting SQL queries, including multi-table joins, subqueries, and window functions, to extract, transform, and analyze complex datasets for actionable insights. Crafted SQL queries for Data Modeling and database design, including multi-table joins, subqueries, and window functions, to extract, transform, and analyze complex datasets for actionable insights. Employed Python scripts for meticulous data cleaning, incorporating regular expressions, custom functions, and statistical techniques to ensure data accuracy and maintain high data quality standards. Expertise in developing and managing end-to-end ETL pipelines, optimizing database performance, and automating reporting processes, resulting in a 20% increase in data-driven decision-making and a substantial reduction in manual reporting efforts. Proficiently engineered Python scripts for data processing and ETL (Extract, Transform, and Load) operations, optimizing data workflows for both efficiency and accuracy. Strong expertise in Model Evaluation and Statistics, enabling sophisticated data-driven insights and informed decision-making with a focus on data warehousing best practices. Proficient in advanced ML/Statistics Algorithms, encompassing Decision Trees, Random Forest, Predictive Modeling, Regression, Classification, Clustering and Time Series Analysis contributing to robust data modeling solutions. Leveraged the capabilities of Python packages such as NumPy, Pandas, Seaborn, and Matplotlib to execute intricate data manipulations and in-depth analysis, contributing significantly to data-driven insights and optimizing data warehousing strategies. Developed Python scripts to automate data cleaning and reporting tasks, streamlining data management processes. Crafted impactful visualizations using Power BI, Tableau, and Excel, enabling stakeholders to make informed decisions based on compelling data representations, while ensuring optimal data warehousing integration. Proficient in designing, developing, and deploying data integration solutions. Utilized SQL for data transformations and seamlessly integrated DataStage with data warehouses within the ecosystem. Demonstrated proficiency in cloud technologies, including Azure Data Factory, Azure Databricks, and AWS CloudFormation, with a focus on streamlining data workflows, optimizing query performance, and implementing real-time data processing solutions. Leveraged cloud technologies like Azure Data Factory and Azure Databricks to design and automate data pipelines. Utilized Databricks for comprehensive data pipeline development and automation, encompassing cluster management, library implementation, and PySpark development for data transformations and integration. Demonstrated expertise in Azure Databricks configuration and administration, ensuring optimal resource utilization and alignment with ETL/ELT processes.
        </p>
      </div>
    </div>
    <div class="section">
      <div class="section-header">
        <h2>Core Qualifications</h2>
      </div>
      <div class="section-body">
        <ul class="qualification-list">
          <li>Programming Languages: Python, Scala, SQL, PySpark, Go, PowerShell & T-SQL</li>
          <li>Hadoop Components / Big Data: PySpark, Airflow, HDFS, MapReduce, Hive, HBase, Sqoop, Impala, Zookeeper, Kafka & Yarn</li>
          <li>Cloud Platform: AWS (Lambda, S3, EC2, EMR, RDS), Microsoft Azure (Azure Databricks, Azure Data Factory, Azure Data Explorer, Azure HDInsight, ADLS), and GCP (Bigquery, Dataflow, Pub/sub)</li>
          <li>Reporting and ETL Tools: AWS GLUE, Tableau & Power BI</li>
          <li>Databases: Oracle, SQL Server, MS Access & NoSQL Database (HBase, MongoDB, DynamoDB)</li>
          <li>Big Data Technologies: Hadoop, HDFS, Hive, Oozie, Sqoop, Spark, Machine Learning, Pandas, NumPy, Seaborn, Impala, Zookeeper, Airflow, Informatica, Snowflake, Data Bricks, Kafka & Cloudera</li>
          <li>Data Analysis Libraries: Pandas, NumPy, SciPy, Scikit-learn & Matplotlib</li>
          <li>Containerization: Docker & Kubernetes</li>
          <li>CI/CD Tools: Jenkins, Ansible, GitLab & Bamboo</li>
          <li>Monitoring Tools: Quicksight, Data Dog, Tableau desktop</li>
          <li>Software Methodologies: Agile, Scrum & Waterfall</li>
          <li>Development Tools: Eclipse, PyCharm, IntelliJ, SSMS & Microsoft Office Suite (Word, Excel, PowerPoint)</li>
          <li>Version Control: GitHub, Gitlab</li>
        </ul>
      </div>
    </div>
    <div class="section">
      <div class="section-header">
        <h2>Education</h2>
      </div>
      <div class="section-body">
        <p>Southern Arkansas University <br> Master's: Computer & Information Science <br> Jan 2021-May 2022</p>
      </div>
    </div>
    <div class="section">
      <div class="section-header">
        <h2>Work Experience</h2>
      </div>
      <div class="section-body">
        <h3>Data Engineer</h3>
        <p>TEKsystems, Charlotte, NC <br> Dec 2022 - Present</p>
        <ul class="qualification-list">
          <li>Employed SQL and Python to execute comprehensive data analysis, applying techniques such as data visualization, data mining, and data warehousing, which resulted in extracting valuable insights from diverse datasets, contributing to a 20% increase in data-driven decision-making. Built and utilized data warehouses for efficient data storage and retrieval.</li>
          <li>Conducted Exploratory Data Analysis (EDA) using Matplotlib and Seaborn, diligently maintained and monitored adherence program reporting, designed, experimented with, and tested hypotheses, and expertly applied advanced statistical and predictive modeling techniques. These efforts led to real-time decision-making improvements by 15%.</li>
          <li>Developed solution-oriented views and dashboards within Power BI, integrating various chart types, including Pie Charts, Bar Charts, Tree Maps, Circle Views, Line Charts, Area Charts, and Scatter Plots, resulting in a remarkable 30% enhancement in data accessibility and understanding among stakeholders. These visualizations relied heavily on data models defined within the data warehousing system.</li>
          <li>Installed and configured Apache airflow for workflow management and created workflows in python.</li>
          <li>Developed a detailed project plan and helped manage the data conversion migration from the legacy system to the target snowflake database.</li>
          <li>Drove the implementation of new processes to streamline cloud infrastructure management and optimize resource allocation.</li>
          <li>Have Good Experience in Enterprise Data Lake (EDL) Big Data Integration Projects using Snap Logic.</li>
          <li>Experience in creating methodologies and technologies that depict the flow of data within and between application systems and business functions/operations & data flow diagrams.</li>
          <li>Worked on Big Data Integration and Analytics based on Hadoop, SOLR, Spark, Kafka, Storm, and web Methods technologies.</li>
          <li>Used cloud shell SDK in GCP to configure the services Data Proc, storage, Big Query.</li>
          <li>Evaluated effectiveness of current pricing architecture and developed recommendations for refinement or to capture new opportunities.</li>
          <li>Design and Build CICD Pipelines for Google Cloud Platform (GCP) services: BigQuery, DataFlow, Pub/Sub, Data Fusion, and others.</li>
        </ul>
      </div>
      <div class="section-body">
        <h3>Big Data Engineer III</h3>
        <p>Comcast, New York <br> June 22 - Dec 2022</p>
        <ul class="qualification-list">
          <li>Collaborated closely with a cross-functional team to design and develop secure, high-performance APIs and data pipelines. Successfully translated user needs into robust data solutions, optimizing ETL processes and data quality within the complex regulations and requirements of the banking and finance industry.</li>
          <li>Leveraging Spark Streaming and Kafka, achieved real-time ingestion of high-volume data from diverse sources, enabling immediate identification and mitigation of potential fraudulent transactions and market fluctuations. This resulted in a 20% reduction in risk detection latency, allowing to react to threats faster and minimize potential losses.</li>
          <li>Extracted and refined technical requirements and implemented agile methodologies to translate user needs into robust pipeline designs and solutions.</li>
          <li>Developed and managed Azure Data Factory, for comprehensive ETL orchestration, incorporating Blob storage for efficient data persistence and backup on the Azure platform.</li>
          <li>Designed and automated ETL pipelines using Databricks for streamlined data processing, ensuring effective workflow management. Utilized Python-based Spark applications to establish distributed environments for loading high-volume files using PySpark into Azure SQL DB tables.</li>
          <li>Implemented streaming pipelines via Azure Event Hubs and Stream Analytics, facilitating data-driven workflow analysis.</li>
          <li>Developed data workflows utilizing Databricks, Scala, and Spark, capturing data from Delta tables in Delta Lakes, contributing to robust ETL processes.</li>
          <li>Created ETL Mapping with Talend Integration Suite to pull data from Source, apply transformations, and load data into target database.</li>
          <li>Created Talend jobs to copy the files from one server to another and utilized Talend FTP Components.</li>
          <li>Developed Spark Streaming scripts for real-time processing, enhancing data accuracy by consuming topics from Kafka. Leveraged Azure Data Factory with Blob storage for storage and backup, employing Python scripting and tools like Airflow for batch data and Kafka for streaming data, building robust ETL pipelines.</li>
          <li>Operated ETL processes using Azure Databricks, employing Kafka for connecting to relational databases.</li>
          <li>Automated data ingestion, transformation, and storage using Apache Spark and Delta Lake, ensuring data quality and integrity.</li>
          <li>Implemented a distributed stream processing platform for low-latency data integration inside and outside Azure, facilitating real-time ETL capabilities.</li>
          <li>Deployed Sqoop for data extraction from Teradata to HDFS and subsequent analysis of patterns back to Teradata.</li>
          <li>Took charge of identifying and implementing SQL Server enhancements to optimize query performance and ensure data integrity.</li>
          <li>Enhanced Hive queries using best practices, Hadoop, YARN, Python, and PySpark to optimize query performance.</li>
        </ul>
      </div>
      <div class="section-body">
        <h3>Data Engineer</h3>
        <p>Apex CoVantage - Em Raaga Informatics, Herndon, VA <br> May 2017 - Dec 2020</p>
        <ul class="qualification-list">
          <li>Design and develop end-to-end data pipelines, utilizing AWS services such as Lambda for serverless computing, ensuring efficient and reliable data movement and transformation.</li>
          <li>Leverage Snowflake's cloud-native data warehousing capabilities to design, build, and manage scalable and performant data storage solutions that align with business needs.</li>
          <li>Develop and implement ETL processes using AWS Glue or custom Python scripts to extract, transform, and load data from various sources into Snowflake for further analysis.</li>
          <li>Craft and optimize complex SQL queries to retrieve and manipulate data efficiently within Snowflake's data warehouse environment, ensuring optimal query performance.</li>
          <li>Utilize AWS Lambda to process and analyze real-time data streams, enabling timely insights and actions based on dynamic data sources.</li>
          <li>Develop Python scripts to automate data-related tasks, perform data validations, and execute data transformations, contributing to the efficiency and accuracy of data workflows.</li>
          <li>Implement data quality checks, validations, and monitoring processes to ensure the integrity, consistency, and accuracy of data stored in Snowflake.</li>
          <li>Integrate data from diverse sources, both internal and external, using AWS services like Data Pipeline, Lambda, or other suitable methods, ensuring seamless data flow.</li>
          <li>Design automated data loading processes that efficiently handle data updates, inserts, and deletes within Snowflake's data warehouse, ensuring data is kept up to date.</li>
          <li>Implement security best practices and ensure compliance with data governance standards while handling sensitive data within AWS and Snowflake environments.</li>
          <li>Identify and resolve data-related issues promptly, applying debugging skills and root cause analysis to maintain the reliability of data engineering solutions.</li>
          <li>Analyzed the system for new enhancements/functionalities and performed Impact analysis of the application for ETL changes.</li>
        </ul>
      </div>
    </div>
    <div class="section">
      <div class="section-header">
        <h2>Certifications</h2>
      </div>
      <div class="section-body">
        <ul class="qualification-list">
          <li>Microsoft Certified: Azure Developer Associate</li>
          <li>Certified in AWS Academy Data Analytics by AWS Academy Graduate</li>
        </ul>
      </div>
    </div>
  </div>
</body>
</html>
